Team: ECNU_ICA
=================Algorithm==================
Run-1(w2v):
We compute the similarities between a query and abstracts of its pids. Either a query or an abstract can be represented as a collection of words, w={w1,w2,...,wn}.
Each word here is a vector. And a query or an abstract can be represented as a vector which is mean value of w.
Assume that we have a query q and its pids,P={p1,p2,...,pm}. We compute similarities between q&p1,q&p2,etc. Finally we rank according to these similarities.
Run-2(l2r):
We use learning to rank model on this task. We extracted the weighting score and the rank of each document-query pair from a retrieval model as features. To utilize the 
advantages of different retrieval models, we obtain the score and rank from BM25, PL2 and BB2 model. Hence, the dimension of feature vector is 6 in this experiment.
We apply random forest to classify the document-query pairs into relevant or not. Each document-pair which is relevant will award an extra relevance score. The result is 
re-ranked by the new score.
Run-3:
Combination of Run-1 and Run-2. The final score of a document to a query contains 2 parts, w2v and l2r. The score of w2v occupies 30% while the score of l2r has the left.
If a document dosen't occur in the l2r but in w2v, the final score will equals to w2v score.
================Data Preprocessing==========
We downloaded title and abstract of each pid on PubMed.
We also processed topic files.
These pre-processed data could be found on Google drive:
The processed downloaded data: https://drive.google.com/open?id=0B1SN0e3hyW9hNU5fMkFNQ1Uwbm8
The processed topic files: https://drive.google.com/open?id=0B1SN0e3hyW9hQmtDSUVOM2hIMkE
There are 3 versions of processed topic files:
1. topic_processed.zip: basic version. just remove some words.
2. topic_processed2.zip & topic_processed3.zip: 2 different formats of a version. This version takes negation into consideration.
    they are used for w2v and l2r.
